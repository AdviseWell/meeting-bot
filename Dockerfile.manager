# Meeting Bot Manager
# Python 3.11 with ffmpeg for media conversion
# Processes individual meeting recordings with media conversion and upload

FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    curl \
    procps \
    && rm -rf /var/lib/apt/lists/* \
    && ffmpeg -version > /dev/null 2>&1 || echo "Warning: ffmpeg installation may have issues"

# Set working directory
WORKDIR /app

# Copy requirements first for better caching
COPY manager/requirements.txt .

# Optional offline diarization deps (Torch + SpeechBrain + sklearn)
# Kept separate so non-diarization builds can cache independently.
COPY manager/requirements-offline-diarization.txt ./requirements-offline-diarization.txt

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Install offline diarization deps (safe even if unused at runtime)
RUN pip install --no-cache-dir -r requirements-offline-diarization.txt

# --- Prebake diarization model artifacts (optional) ---
# The offline diarization path in scripts/whisper_diarize.py expects a local
# SpeechBrain model directory at:
#   /app/tools/speechbrain/spkrec-ecapa-voxceleb/
# containing hyperparams.yaml + embedding_model.ckpt (+ optional normalizer).
#
# If you want a fully offline image, place the model files in the build context
# at:
#   tools/speechbrain/spkrec-ecapa-voxceleb/
# and they will be copied into the image here.
RUN mkdir -p /app/tools/speechbrain/spkrec-ecapa-voxceleb
COPY tools/speechbrain/spkrec-ecapa-voxceleb/ /app/tools/speechbrain/spkrec-ecapa-voxceleb/

# If the repository contains only a partial set of SpeechBrain assets, download
# the remaining required files at *build time* so runtime stays offline.
#
# This downloads from Hugging Face during the image build (Option B), not at
# runtime.
RUN python - <<'PY'
import os
from pathlib import Path

model_dir = Path('/app/tools/speechbrain/spkrec-ecapa-voxceleb')
model_dir.mkdir(parents=True, exist_ok=True)

required = [
    'hyperparams.yaml',
    'embedding_model.ckpt',
    'mean_var_norm_emb.ckpt',
    'classifier.ckpt',
    'label_encoder.txt',
]

missing = [f for f in required if not (model_dir / f).exists()]
if not missing:
    print('SpeechBrain diarization assets already present; skipping HF download.')
    raise SystemExit(0)

print('Missing diarization assets:', missing)
print('Downloading missing SpeechBrain diarization assets from Hugging Face (build time)...')

from huggingface_hub import hf_hub_download

repo_id = os.environ.get('SPEECHBRAIN_HF_REPO', 'speechbrain/spkrec-ecapa-voxceleb')

for filename in missing:
    path = hf_hub_download(
        repo_id=repo_id,
        filename=filename,
        local_dir=str(model_dir),
        local_dir_use_symlinks=False,
    )
    print('Downloaded', filename, '->', path)

print('SpeechBrain diarization assets download complete.')
PY

# --- Build whisper.cpp + download model (reliable for CI multi-arch) ---
# Expected by manager/offline_pipeline.py defaults:
#   /app/tools/whisper.cpp/build/bin/whisper-cli
#   /app/tools/whisper.cpp/models/ggml-base.en.bin
RUN mkdir -p /app/tools/whisper.cpp/build/bin /app/tools/whisper.cpp/models \
        /app/tools/whisper.cpp/build/src /app/tools/whisper.cpp/build/ggml/src

# Build whisper-cli + shared libs inside the image so multi-arch builds work.
RUN apt-get update && apt-get install -y --no-install-recommends \
            build-essential \
            cmake \
            git \
            ca-certificates \
        && rm -rf /var/lib/apt/lists/* \
        && git clone --depth 1 https://github.com/ggerganov/whisper.cpp /tmp/whisper.cpp \
        && cmake -S /tmp/whisper.cpp -B /tmp/whisper.cpp/build -DCMAKE_BUILD_TYPE=Release \
        && cmake --build /tmp/whisper.cpp/build --target whisper-cli -j"$(nproc)" \
        && cp /tmp/whisper.cpp/build/bin/whisper-cli /app/tools/whisper.cpp/build/bin/ \
        && if ls /tmp/whisper.cpp/build/src/libwhisper.so* >/dev/null 2>&1; then cp /tmp/whisper.cpp/build/src/libwhisper.so* /app/tools/whisper.cpp/build/src/; fi \
        && if ls /tmp/whisper.cpp/build/ggml/src/libggml*.so* >/dev/null 2>&1; then cp /tmp/whisper.cpp/build/ggml/src/libggml*.so* /app/tools/whisper.cpp/build/ggml/src/; fi \
        && rm -rf /tmp/whisper.cpp \
        && apt-get purge -y --auto-remove build-essential cmake git || true

# Download model into the image.
RUN if [ ! -f /app/tools/whisper.cpp/models/ggml-base.en.bin ]; then \
            echo "Downloading whisper model ggml-base.en.bin"; \
            curl -fsSL -o /app/tools/whisper.cpp/models/ggml-base.en.bin \
                https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin; \
        fi

# Copy manager application
COPY manager/ .

# Create directory for temporary recordings
RUN mkdir -p /tmp/recordings

# Run as non-root user for security
RUN useradd -m -u 1000 manager && \
    chown -R manager:manager /app /tmp/recordings
USER manager

# Set Python to run in unbuffered mode for better logging
ENV PYTHONUNBUFFERED=1

# Ensure Hugging Face / Torch / SpeechBrain caches go to a writable location.
# Running as non-root means '/.cache' is not writable and will cause failures
# when SpeechBrain attempts to fetch or read cached artifacts.
ENV XDG_CACHE_HOME=/tmp/.cache
ENV HF_HOME=/tmp/.cache/huggingface
ENV TRANSFORMERS_CACHE=/tmp/.cache/huggingface/transformers
ENV HUGGINGFACE_HUB_CACHE=/tmp/.cache/huggingface/hub
ENV TORCH_HOME=/tmp/.cache/torch

# Ensure our optional `sitecustomize.py` (torchaudio/SpeechBrain compat) is on
# sys.path at interpreter startup.
ENV PYTHONPATH=/app

# Make sure whisper-cli can locate its shared libraries.
ENV LD_LIBRARY_PATH=/app/tools/whisper.cpp/build/src:/app/tools/whisper.cpp/build/ggml/src

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import sys; sys.exit(0)"

# Run the manager
CMD ["python", "-u", "main.py"]
